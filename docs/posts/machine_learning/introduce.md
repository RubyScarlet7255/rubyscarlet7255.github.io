---
head:
  - - link
    - rel: stylesheet
      href: https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css

title: 《机器学习绪论》
date: 2026-01-17 15:00:00
categories: 机器学习
cover: [/pics/machine_learning/normal_bayes_and_logistic_regression/cover.jpg]
sticky: 0

tags: ["机器学习","AI","读书笔记"]

---

# 概论

## 统计学习

&emsp;&emsp;统计学习（statistical learning）是基于数据构建概率模型，并且以用模型对数学进行预测分析的一门学科，统计学习也被称为统计机器学习（statistical machine learning）。

&emsp;&emsp;统计学习关于数据的基本假设是**同类数据具有一定的统计规律性**，这也是统计学习的前提。而正是因为如此，这些数据可以通过概率统计方法进行处理。

&emsp;&emsp;对数据的预测与分析是统计学习的主要应用，而这应用也正是基于对于数据构建的概率模型实现的。

&emsp;&emsp;其可以概括如下：从给定的、有限的、用于学习的**训练数据集合（training data）**出发，假设数据是**独立同分布**产生的；并且要学习的模型属于某个函数（通俗一点即数据之间映射关系）的集合，该集合称为**假设空间（hypothesis space）**;并且应用某个**评价准则（evaluation criterion）**。统计学习的目标就是从**假设空间**中，依照**选定训练集**以及**评价准则**，最终选取一个**最优模型**。

&emsp;&emsp;因此，统计学习可以分为三要素：
1. 模型（model）
2. 策略（strategy）：学习的目标和准则
3. 算法（algorithm）：具体如何实现目标

&emsp;&emsp;可以将统计学习的步骤总结如下：
1. 得到一个有限的训练集
2. 确定包含所有可能的模型的假设空间，即学习模型的集合
3. 确定模型选择的准则，即模型的策略
4. 实现求解最优模型的算法
5. 通过**学习方法**选择最优模型（补充：学习方法，在统计学习中通常定义为：一个系统能够通过执行某个过程以改进它的性能）
6. 利用得到的最优模型对数据进行预测分析

### 重要名词

- **输入空间、特征空间、输出空间**

&emsp;&emsp;输入空间（input space）与输出空间(output space)指的是输入输出所有可能的集合，其可以是优先或者无限的集合，二者可以处于同一或者不同空间。

&emsp;&emsp;每一个输入都是一个实例（instance），而在学习过程中，需要根据所研究的问题实例中提取出计算机能够识别的**特征向量(feature vector)**。所有特征向量存在的空间称为**特征空间**。特征空间的每一个维度，对应着实例的一个特征。

- **假设空间**

&emsp;&emsp;模型是由输入空间到输出空间映射的集合，而这个集合就是**假设空间（hypothesis space）**，学习的目的就是从假设空间中找出最优模型。

## 统计学习的分类

### 监督学习（supervised learning）

&emsp;&emsp;监督学习从标注数据中学习，标准数据包含明确的输入与输出的对应关系。监督学习的本质是学习输入空间到输出空间映射的统计规律。

&emsp;&emsp;监督学习可以分为学习和预测两个过程。通过给定一个训练集，得到一个模型；再根据对于给定的测试样本集进行评价。

### 无监督学习（unsupervised learning）

&emsp;&emsp;无监督学习从无标注的数据集中进行学习，其本质是学习数据中的统计规律或者潜在结构。其依照评价准则而非标注数据给出的对应关系，从假设空间中选出最优模型。

&emsp;&emsp;无监督学习也分为学习和预测两个过程。

### 强化学习（reinforcement learning）

&emsp;&emsp;强化学习是通过与环境的连续互动中学习最优行为策略的机器学习问题，其核心要素为智能系统和环境。

&emsp;&emsp;其工作方式简单概括如下：智能体根据当前检测到的**环境状态**，与之前获得**分数**，采取一个**动作**。环境根据智能系统出去的动作，得到**下一步状态**，以及给智能系统**分数反馈**。强化学习的目标是通过长期积累奖励得到分数的最大化，其通过不断试错从而学习最优战略。

### 半监督学习与主动学习

&emsp;&emsp;**半监督学习（semi-supervised learning）** 所使用的数据集包含标注与未标注的数据集。在标注数据集中，标记往往是人工提供的，对于大批量数据来说成本过高。半监督学习利用未标注的数据学习，同时辅以标注数据进行监督学习。

&emsp;&emsp;**主动学习（active learning）** 是指机器不断自动给出实例让外界对实例进行标注，然后利用标注的数据学习预测模型的机器学习问题。其目标是找出对学习最优帮助的实例让人工标注，从而以较小的标注代价达到较好的学习效果。

## 统计学习方法分类

### 模型的分类
&emsp;&emsp;以下是按照统计学习模型的不同分类方式。

- **概率模型与非概率模型**
&emsp;&emsp; **概率模型（probabilistic model）** 中，输入为 $x$ ，输出为 $y$，概率模型所采用的模型取条件概率分布形式 $P(y|x)$ 。

&emsp;&emsp; **非概率模型（nonprobabilistic model）** 中，输入为 $x$ ，输出为 $z$ ,模型取条件概率分布形式为 $P(z|x)$ 或者 $P(x|z)$ 。

&emsp;&emsp;这两种模型的区别在于模型的内在结构，概率模型一定可以表示为联合概率分布的形式，而非概率模型不一定存在这样的概率分布。

- **线性模型与非线性模型**

&emsp;&emsp;**线性模型(linear model)**和**非线性模型(non-linear model)** 的划分标准是模型函数 $y=f(x)$ 是否是线性函数。

- **参数化模型和非参数话模型**

$emsp;$emsp;**参数化模型(parametric model)**假设模型参数的维度固定，其可以由有限维度的参数完全刻画。而**非参数化模型（non-parametric model）** 假设的模型参数维度不固定或者无穷大。

### 算法的分类

&emsp;&emsp;按照算法，可以分类为 **在线学习（online learning）** 与 **批量学习（batch learning）**。

&emsp;&emsp;在线学习每次接受一个样本，进行预测，然后进行学习。不断重复该操作得到最终的模型。

&emsp;&emsp;批量学习一次接受所有的数据进行学习，然后进行预测。

### 技巧的分类

- **贝叶斯学习（Bayesian learning）**

&emsp;&emsp;利用贝叶斯独立，计算在给定数据条件下模型的条件概率（即后验概率），从而对数据进行预测。

&emsp;&emsp;设水机变量 $D$ 表示数据,随机变量$\theta$表示模型参数,按照贝叶斯基础公式如下：

$$P(\theta|D)=\frac{P(\theta)P(D|\theta)}{P(D)}$$

&emsp;&emsp;其中 $P(\theta)$ 是先验概率， $P(D|\theta)$ 是似然函数。在估计模型的时候，估计的是整个后验分布。根据样本 $x$ 预测时，计算数据对后仰概率分布的期望值：

$$P(x|D) = \int P(x|\theta,D)P(\theta|D)d\theta$$

- **核方法（kernel method）**

&emsp;&emsp;核方法是使用和函数表示核学习非线性模型的一种机器学习方法。把线性模型拓展到非线性模型最直接的做法，是显式定义从输入空间到特征空间的映射，然后再特征空间进行内积运算。核方法通过直接定义**核函数**，实现将输入空间映射到特征空间的内积运算。

&emsp;&emsp;假设 $x_1$ 和 $x_2$ 是输入空间的任意两个实例（向量），其内积为 $<x_1,x_2>$ 。假设从输入空间到特征空间的映射函数为 $\phi$，其再特征空间上的内积就是 $<\phi(x_1), \phi(x_2)>$。核方法即直接定义核函数，以实现 $K(x_1,x_2)=<\phi(x_1),(x_2)>$ 。

## 统计学习三要素

### 模型

&emsp;&emsp;模型就是要学习的条件概率分布或者决策函数，所有可能的模型组成的集合称为假设空间。

### 策略

&emsp;&emsp;策略即如何在假设空间中选取最优的模型。此处说明两个最基础的策略。

- **损失函数和风险函数**

&emsp;&emsp;对于给定的输入 $X$ ，通过当前得到的模型 $f(X)$ 得到相应的输出 $Y$ 。这个预测值和真实值通常存在差异，因此使用一个**损失函数（cost function）** 来度量预测错误的长度，记为 $L(Y,f(x))$ ，其是非负实值函数。当损失函数得到的值越少，说明这个模型越好。

&emsp;&emsp;对于模型的输入、输入 $(X,Y)$ 是随机变量，这些随机变量遵循联合分布 $P(X,Y)$ ，所以损失函数的期望如下：

$$R_{exp}(f) = E_{P}[L(Y,f(X))] = \int L(y, f(x))P(x,y)dxdy$$

&emsp;&emsp;这个损失函数也被称为**风险函数(risk function)**或者**期望损失（expected loss）**。

&emsp;&emsp;风险函数是期望下的数据，在有限的样本中我们使用 **经验风险(empirical risk)** 或者 **经验损失（empirical loss）**，记作 $R_{emp}$：

$R_{emp}(f) = \frac{1}{n} \sum^{N}_{i=1}L(y_i,f(x_i))$

&emsp;&emsp;根据大数定律，样本量趋近于无穷时，二者会趋近。

- **经验风险最小化与结构最小化**

&emsp;&emsp;由于现实中能够使用的训练样本数量有限，所以使用经验风险估计期望存在一定缺陷。

&emsp;&emsp;在假设空间、经验损失以及训练集确定的情况下，经验风险函数是可以确认的。**经验风险最小化（empirical risk minimization，ERM）** 的策略认为，经验风险最小的模型就是最优的模型。根据这一个策略，其就是求最优化问题：

$$min_{f\in{F} }\sum^{N}_{i=1}L(y_i,f(x_i))$$

&emsp;&emsp;样本容量足够大时，其有较好的学习效果，而容量很小的时候，容易产生 **过拟合（over-fitting）** 现象。

&emsp;&emsp;**结构风险最小化（structural risk minimization，SRM）** 是为了防止过拟合的策略，其等价于 **正则化（regularization）**。其是在经验风险上添加表示模型复杂度的 **正则化项（regularization）** 或者 **罚项（penalty term）** 。在假设空间，损失函数以及询练数据集确定的情况下，结构发现定义如下：

$$R_{srm}(f)=\frac{1}{N}\sum^{N}_{i=1}L(y_i,f(x_i)) + \lambda J(f)$$

&emsp;&emsp;$J(f)$表示模型的复杂度，模型越复杂，其越大。$\lambda \leq 0$ 用来权衡经验风险和模型复杂度。

&emsp;&emsp;风险结构最小化就是找结构风险最小的模型。

$$min_{f\in{F} }\frac{1}{N}\sum^{N}_{i=1}L(y_i,f(x_i)) + \lambda J(f)$$

### 算法

&emsp;&emsp;算法是学习模型的具体计算方法，求解最优模型有找解析解以及全局最优解两种方式。通常情况下解析解是在不到的，而此时需要使用数值方法取计算全局最优解，并且期望求解过程高效。

## 模型评估与模型选择

### 训练误差与测试误差

&emsp;&emsp;基于损失函数的模型的 **训练误差（training error）** 和**测试误差（test error）** 是学习方法的重要评估标准。

&emsp;&emsp;假设学习到的系统模型是 $Y = \hat{f}(x)$ 。

&emsp;&emsp;训练误差就是这个模型关于训练数据集的平均损失。

$$R_{emp}(\hat{f}) = \frac{1}{N}\sum^{N}_{i=1}L(y_i,  \hat{f}(x))$$

&emsp;&emsp;测试误差就是这模型关于测试数据集的平均损失。注：在学习和评估时，不一定需要采取相同的损失函数。

$$e_{test}=\frac{1}{ N^{'} } \sum^{N^{'} }_{i=1}\hat{L}(y_i, \hat{f}(x_i))$$

### 过拟合和模型选择

&emsp;&emsp;有时，假设空间含有不同复杂度的模型，我们希望选择一个合适的模型。我们称最为理想的模型为**真模型** ，在学习时，我们期望做的就是不断朝向真模型逼近。

&emsp;&emsp;如果一昧追求对训练数据的预测能力，所采用的模型复杂度会远高于真模型。这个现象就是**过拟合（over-fitting）** 。具体一点解释，过拟合即学习时选择的模型参数过多，以至于只对已知的数据预测优秀，但对未知数据预测表现很差。真实的样本数据点会存在噪声，当完美契合训练集时，这个模型也受到了噪声的干扰。因此，在模型选择的时候，不仅要考虑对已知数据的预测能力，还要考虑对未知数据的预测能力。

&emsp;&emsp;当模型复杂度增加时，学习时的训练误差会逐渐减小并且，而测试误差会出现先减小后增大的趋势。

## 正则化和交叉验证

### 正则化（regularization）

&emsp;&emsp;正则化是一个典型的模型选择方式。正则化是结构风险最小化的实现方式，即在经验风险上加一个正则化项或者罚项。正则化项一般是模型复杂度的单调递增函数，模型越大，该项值越大。从贝叶斯估计的角度看，正则化项对应于模型的先验概率。

### 交叉验证（cross validation）

&emsp;&emsp;交叉验证是另一种模型选择方式。

&emsp;&emsp;如果数据充足，可以将数据集分为**训练集（training set）** ，**验证集（validation set）** ，和 **测试集（test set）** 三部分。训练集由于模型训练，验证集用于模型选择，测试集用于模型评估。一般而言，在学习到的不同复杂度的模型中，选择对验证集有最小误差预测误差的模型。

&emsp;&emsp;但是实际上，数据往往是不构充足的，这时候可以采用交叉验证。其思想就是重复的使用数据，将数据切分，将切分好的数据集组合为训练集和测试集，并且在此基础上反复训练，测试，以及模型选择。

- **简单交查验证**

&emsp;&emsp;将已给的数据分为两部分，一部分作为训练集，另一部分作为测试集。用训练集在不同的条件（如模型复杂度）下学习，从而达到不同的模型。在测试集上对各个模型进行测试，选择误差最小的。

- **S折交查验证（S-fold cross validation）**

&emsp;&emsp;实际将已给的数据分为S个互不相交，相同大小的子集。然后利用S-1个子集进行训练，留一个进行测试。可以将S个子集轮流作为测试集进行验证，最终选出测试误差最小的模型。

- **留一交叉验证（leave-one-out cross validation）**

&emsp;&emsp;留一交叉验证是S折交叉验证的特殊情况。$N$ 是给定的书籍及容量，其是 $S=N$ 的S折交叉验证。

## 泛化能力（generalization ability）

&emsp;&emsp;学习方法的泛化能力是指的改方法学习到的模型对于未知数据的预测能力，是学习方法本质上重要的性质。一般是通过测试集来评价泛化能力，然而这种评价依赖于测试数据集，结果很可能是不可靠的。

### 泛化误差（generalization error）
&emsp;&emsp;如果学习到的模型为 $\hat{f}$ ,那么使用这个模型对于**未知数据** 预测出来的误差就成为泛化误差。

### 泛化误差上界（generalization error bound）

&emsp;&emsp;学习方法的泛化能力分析往往是通过研究泛化误差概率上界进行的。通过比较两种学习方法的泛化误差上界大小来比较它们的优劣，其具有以下性质：

1. 其是样本容量的函数，当样本足够大时，泛化上界趋近于0。
2. 其是假设空间容量的函数，假设空间越大，模型越难学，泛化误差上界越大。

## 生成模型和判别模型

&emsp;&emsp;监督学习方法可以分为 **生成方法（generative approach）** 和 **判别方法（discriminative approach）** 。这二者学习达到的模型分别称为 **生成模型（generative model）** 和 **判别模型（discriminative model）** 。

### 生成方法

&emsp;&emsp;生成方法根据数据学习联合概率分布 $P(X,Y)$ ,然后求出条件概率分布 $P(Y|X)$ 作为预测的模型。折描述了给定输入 $X$ 产生输出 $Y$ 的生成关系。

&emsp;&emsp;其特点在于可以还原出联合概率分布 $P(X,Y)$ ，并且收敛速度很快。模型的收敛速度会随着样本容量增加而增加。同时如果存在隐变量，也仍然可以采用生成方法学习。

### 判别方法

&emsp;&emsp;判别方法直接由数据学习决策函数 $f(X)$ 或者条件概率分布 $P(Y|X)$ 作为预测的模型。其关心的是给定输入 $X$ ，应该预测出什么样的输出 $Y$ 。

&emsp;&emsp;判别方法直接学习条件概率或者决策函数。在直接面对预测时候，其学习准确率往往更高。并且用于其直接学习条件概率或者决策函数，可以对数据进行各种程度上的抽象，特征定义，并且使用特征。所以可以简化学习问题。

## 监督学习应用

### 分类问题

&emsp;&emsp;分类问题是监督学习的一个核心问题，监督学习从数据中学到一个分类模型或者分类决策函数，称为 **分类器(classifier)** ，分类器对新的输入进行输出的预测则称为 **分类（classification）** 。

### 标注问题

&emsp;&emsp;**标注（tagging）** 也是一个监督学习的问题，可以认为其是分类问题的一个推广，其又是更加复杂的**结构预测（structure prediction）** 问题的简单形式。其目标在于学习一个模型，使它能够对观测到的序列给出标记序列作为预测。

### 回归问题

&emsp;&emsp;**回归（regression）** 用于预测输入变量（自变量）和输出变量（因变量）之间的关系，尤其是输入变量值变化时，输出变量的值发生的变化，其等价于函数拟合。