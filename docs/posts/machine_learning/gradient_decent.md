---
head:
  - - link
    - rel: stylesheet
      href: https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css

title: 梯度下降法
date: 2025-6-24 22:00:00
categories: 机器学习
cover: [/pics/gradient_decent/cover.jpg]
sticky: 0

tags: ["机器学习", "AI", "数学", "算法"]
---

![封面](/pics/gradient_decent/cover.jpg)

# 梯度下降法

&emsp;&emsp;梯度下降是一种进行模型损失函数最小优化的常用算法，虽然本文标题为梯度下降，但是实际上能够实现如此收敛的常用算法比如牛顿法和拟牛顿法也会在本文进行笔记。遇到具体情况的时候仍然需要结合函数本身选择合适的策略。

## 梯度下降法

&emsp;&emsp;梯度下降法是一种基于一阶连续偏导的优化方式，其要求解的无约束最优化问题是：
$$\min_{x \in R^n} f(x)$$

&emsp;&emsp;在每一次优化时，都选取一个适当的初值 $x^{(0)}$，沿着 $f(x^{(0)})$ 的负梯度方向，不断迭代更新 $x$ 的值，直到收敛。其在凸函数情况下往往能找到局部最优解，但是很多时候并不保证函数满足凸函数的条件，也因此收敛速度未必快。

&emsp;&emsp;其算法如下：

1. 输入：目标函数 f(x), 梯度函数 $g(x) = \delta f(x)$，精度 $\epsilon$
2. 选择初始 $x^{(0)} \in R^n$ ，置迭代次数 k = 0
3. 计算当前位置的梯度值 $g_k = g(x^{(k)})$，当 $g_k$ 绝对值小于 $\epsilon$ 时听制迭代，此时的 $x^{(k)}$ 即为我们的局部最优解。否则令 $p_k = -g(x^{(k)})$ ，求解步长 $\lambda_k$

$$f(x^{(k)}) + \lambda_{k}p_{k} = \min_{\lambda \geq 0} f(x^{(k)} + \lambda p_k )$$

4. 置 $x^{(k+1)} = x^{(k)} + \lambda_k p_k$，同时求解 $f(x^{(k+1)})$，若 $f(x^{(k + 1)})$ 与初始值的差值小于 $\epsilon$ 那么此时的 $x^{(k+1)}$ 即为输出的局部最优解。

&emsp;&emsp;步长在此处的另一个含义即是学习率，为了得到最终的收敛，学习率需要足够小。一个常见的设定算法为：$\frac{\lambda^0}{k}$

### 自适应梯度下降法 Adagrad

&emsp;&emsp;传统的下降法对于所有特征都使用相同的学习率，但是如果特征尺度差异大或者特征频率差异大，其效果往往较差。Adagrad算法对大梯度特征设置小学习率防止震荡，而对小梯度特征设置大的学习率加快收敛。其算法如下：

1. 初始化参数 $w_0$ 和特征累计向量 $z$，二者都为相同维度的零向量。设置全局学习率 $\alpha$，收敛精度$\sigma$ 和防止出现除以0的极小常数$\epsilon$
2. 计算 $w^{(k)}$ 位置的梯度 $g$，$z_d^{(k+1)} = z_d^{k} + g_d^2$, $w_d^{(k+1)} = w_d^{k} - \alpha \frac{g_d}{\sqrt{z_d+\epsilon}}$。
3. 如果前后 $w$的模的差值小于 $\sigma$ 则收敛，否则重复步骤二。

## 牛顿法和拟牛顿法

&emsp;&emsp;牛顿法和拟牛顿法也是具有较好收敛速度的常用方法。牛顿法通过迭代计算，每一步需要求解目标函数 Hessian 矩阵的逆矩阵。而拟牛顿法则通过正定矩阵近似 Hessian 矩阵或者其逆矩阵，简化了计算。

### Hessian 矩阵

&emsp;&emsp;Hessian矩阵是一个多元函数的二阶偏导构成的矩阵，描述了函数的局部曲率。其构造过程通过两次偏导完成。

&emsp;&emsp;第一次偏导会先针对函数 $f(w)$ 中的 $w$ 每一个维度计算偏导，从而获取梯度向量:
$\delta f$

&emsp;&emsp;第二步则是将梯度向量中的每个原始分别对变量 $w_1$ 到 $w_d$ 求偏导，简写为：
$$H_{ij} = \frac{\partial^2f}{\partial w_i \partial w_j}$$

### 牛顿法

&emsp;&emsp;牛顿法应用了泰勒的二阶展开，并且假定了$f(x)$具有二阶连续偏导，那二阶展开的样子如下：

$$f(x) = f(x^{(k)}) + g_k^T(x-x^{(k)}) + \frac{1}{2}(x-x^{(k)})^TH(x^{(k)})(x-x^{(k)})$$

&emsp;&emsp;牛顿法应用了极小点的必要条件：
$$\nabla f(x) = 0$$

&emsp;&emsp;每次迭代都从点 $x^{(k)}$ 开始，求目标函数的极小点，作为第$k+1$次迭代值$x^{(k+1)}$。

&emsp;&emsp;假设$x^{(k+1)}$ 满足：
$$\nabla f(x^{(k+1)}) = 0$$

&emsp;&emsp;那么又之前的二阶泰勒展开可得到：

$$\nabla f(x) = g_k+H_k(x-x^{(k)})$$

&emsp;&emsp;带入假设的极小点可以得到：
$$g_k + H_k(x^{(k+1)} - x^{(k)}) = 0$$

$$x^{(k+1)} = x^{(k)} - H_k^{-1}g_K$$

&emsp;&emsp;令$p_k = - H_k^{-1}g_K$

$$x^{(k+1)} = x^{(k)} + p_k$$

&emsp;&emsp;而牛顿法的实际算法如下：

1. 输入目标函数 $f(x)$，精度要求 $\epsilon$，梯度向量 $g(x)$ 和黑塞矩阵 $H(x)$。
2. 取初始点$x^{(0)}$，计数 $k = 0$
3. 计算 $g_k = g(x^{(k)})$, 如果 $||g_k|| < \epsilon$，则此时的 $X^{(K)}$ 即为我们需要的近似解。
4. 计算$H_k=H(x^{(k)})$，并且求$p_k$
   $$p_k = - H_k^{-1}g_k$$
5. 置 $x^{(k+1)} = x^(k) + p_k$，$k = k +1$，回到第二部开始重复执行。

&emsp;&emsp;如果近似足够精确并且步数足够小，牛顿法的收敛会非常快。如果函数平坦或者在某些维度上近似平坦，即二阶导数接近于0，那么此时二阶导数的导数会因为非常巨大导致巨量步数，整体难以收敛。

### 拟牛顿法

&emsp;&emsp;在牛顿法中，Hessian矩阵的求逆是非常复杂的过程，而拟牛顿法则是采用一个n阶矩阵$G_k = G(x^{(k)})$ 来近似替代 $H_k^{-1}$

&emsp;&emsp;回到牛顿法的推导，根据：
$$$$\nabla f(x) = g_k+H_k(x-x^{(k)})$$

&emsp;&emsp;可以得到：
$$g_{k+1} - g_{k} = H_k(x^{(k+1)} - x^{k})$$

&emsp;&emsp;令：$y_k = g_{k+1} - g{k}$，$\delta_k = x^{(k+1)} - x^{(k)}$。则：
$$y_k = H_k \delta_k$$

$$H_k^{-1}y_k = \delta_k$$

&emsp;&emsp;上面两个式子也被称为拟牛顿条件。

&emsp;&emsp;在进行接下来的笔记前，先补充一个概念。假设实对称矩阵A满足对于任意非零向量 $\alpha \in R^n$，都有 $\alpha^TA\alpha > 0$，则称A为正定矩阵。正定矩阵具有以下特质：

1. A是正定矩阵等价于矩阵A的特征值全是正数
2. 如果A是正定举证，则存在唯一的正定矩阵S使得$A=S^2$
3. 如果A，B都是正定矩阵，那么AB的特征值都为正数
4. 如果A是正定矩阵，那么$A^{-1}$ 也是正定矩阵

&emsp;&emsp;拟牛顿法并不要求原始的Hessian矩阵是正定矩阵，但是其推导是基于对应Hessian矩阵是正定矩阵的基础之上的。在实际的拟牛顿方法中，会通过构造的方式以在别的方面满足足够假设。

&emsp;&emsp;如果$H_k$是正定矩阵，那么$H_k^{-1}$也是正定矩阵，那么因为$p_k = -H_k^{-1}g_k$，所以可以保证搜索方向 $p_k$是下降方向。因为：

$$x = x^{(k)} + \lambda p_k = x^{(k)} - \lambda H_k^{-1}g_k$$

&emsp;&emsp;所以其二阶泰勒可以近似于：

$$f(x) = f(x^{(k)}) - \lambda g_k^TH_k^{-1}g_k$$

&emsp;&emsp;因为 $H_k^{-1}$ 正定，所以 $\lambda g_k^TH_k^{-1}g_k > 0$$，当$\lambda$为正数时，总有 $f(x) < f(x^{(k)})$。即说明$p_k$就是下降方向。

&emsp;&emsp;通常拟牛顿法将 $G_k$ 作为 $H_k^{-1}$ 近似，也有使用对 $H_k$ 近似的拟牛顿算法。

&emsp;&emsp;对于前者，$G_k$满足：

$$G_{k+1}y_k = \sigma_k$$

&emsp;&emsp;按照拟牛顿条件，每次迭代都可以选择更新矩阵$G_{k+1}$

$$G_{k+1} = G_k + \nabla G_k$$

## 参考

[1]《统计学习方法》(李航)(清华大学出版社)

[2]《机器学习讲义（何琨）》

[3][D老师](https://www.deepseek.com)

[4][高等代数：正定矩阵及其性质](https://zhuanlan.zhihu.com/p/673933967)
